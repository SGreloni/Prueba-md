## Acerca de mí

Soy un estudiante de la carrera de Actuario en Economía en la Universidad de Buenos Aires, con una gran curiosidad y un especial interés por la Ciencia de Datos, la Estadística y la Inteligencia Artificial. 

#### Educación

-**Universidad de Buenos Aires** *(03/2019 - Actualidad)*   
Promedio 8,5 *(28/33 materias)*


-**Colegio Nacional de Buenos Aires** *(03/2014 - 11/2018)*  
Promedio 8,22 


## Curriculum Vitae

[CV](https://drive.google.com/file/d/1et4YOYs3aJySvBQDTHJUBVC_vf2R_G4x/view?usp=sharing)


## Certificiaciones
[Certificaciones](Certificaciones)

## Proyectos

### [Proyecto 1.1:  Análisis y predicción de precios en Airbnb](https://github.com/SGreloni/prediccion-precios-Airbnb/blob/master/Predictor%20de%20precios%20de%20Airbnb%20.ipynb)

Realicé un análisis sobre datos de 20000 propiedades listadas en Airbnb y entrené un modelo de regresión que predice el precio por noche al que debería estar ofrecida la propiedad (en pesos a diciembre de 2020) según características como su capacidad, cantidad de baños, número de dormitorios, etcétera.

Herramientas utilizadas:
* Pandas (limpieza de datos)
* Matplotlib y Seaborn (visualización de datos)
* Scikit-Learn (*feature engineering* y ajuste de hiperparámetros)
* XGBoost (*Machine Learning*: regresión)
* Markdown y LaTex (legiblididad y publicación de textos digitales)

![Disposición de propiedades de Airbnb en CABA](Proyectos%20(img)/Mapa%20airbnb.png)

### [Proyecto 1.2:  Aplicación Web para prediccion de precios de Airbnb](https://sgreloni-prediccion-precios-airbnb-streamlit-7qdwlt.streamlit.app/)

Utilizando el modelo de inteligencia artifical entrenado en **Proyecto 1: Análisis y predicción de precios en Airbnb** construí una aplicación web con una interfaz estética e intuitiva para tasar la estadía por noche (en pesos a diciembre de 2020) según el input obtenido del usuario.

Herramientas utilizadas:
* Streamlit (generación de aplicaciónes Web)
* Geopy (servicios de geocodificación)
* Heroku (despliegue en la nube)
* Git (control de versiones)

### [Proyecto 2: SegArt](https://sgreloni-segart-artapp-nvijob.streamlit.app/)

Utilicé una aplicación no convencional del algoritmo de aprendizaje no supervisado *KMeans* como segmentador de imagen, aprovechando para explicar didácticamente como funciona y permitiendo al lector jugar con el hiperparámetro K (la cantidad de *clusters* que busca el algoritmo). Adicionalmente agregué la opción de poder modificar los colores de cada segmento o cluster con el objetivo de crear composiciones artísticas fácilmente. Para más información acerca del proceso de creación visitar el [repositorio en Github](https://github.com/SGreloni/segart) y el [Jupyter Notebook](https://github.com/SGreloni/segart/blob/master/SegArt.ipynb).

Herramientas utilizadas:
* Numpy (soporte de vectores, matrices y funciones matemáticas)
* Matplotlib (Visualización de datos -2D, 3D y animaciones-)
* KMeans -Sklearn- (*Machine Learning*: Clusterización)
* Markdown (legiblididad y publicación de textos digitales)
* PIL (soporte de manipulación de imágenes)
* Streamlit (generación de aplicaciónes Web)
* Heroku (despliegue en la nube)
* Git (control de versiones)

![Animación 3D](Proyectos%20(img)/rotation.gif)

### [Proyecto 3: Spotify Clustering Challenge](https://github.com/SGreloni/Spotify-Clustering-Challenge/blob/main/Hiring%20Process%20Challenge.ipynb)

This project is my solution to a Challenge which was part of a Hiring Process. I was asked to analyze and cluster a small sample of the Sportify Dataset having into account not only classical metrics such as Inertia but also the intuition and meaning of each cluster. For this I made an Exploratory Data Analysis and tried both DBSCAN and K-Mean on the data. For choosing the definitive clusters I made a Silhouette Diagram Analysis, manual inspection of individual instances and a visualization of the data structure using t-SNE. Finally, to help me interpet the results I fitted a Random Forest Classifier to the clusters to find, with the feature importances, the most important variables in differentiating each group.

Main tools:
* Pandas
* Matplotlib
* Seaborn
* Scikit-learn (DBSCAN, K-Means, Random Forest, t-SNE manifold learning)
* Yellowbrick (Silhouette Diagram Analysis)

![Embedding coloured by cluster](Proyectos%20(img)/t-SNE.png)

### [Economics of Financial Markets Final Project](https://github.com/SGreloni/Finance-Portfolio-Optimization-/blob/main/Suliansky%20and%20Greloni%20(Q1).pdf)

This is the final project we wrote with [Gaspar Suliansky](https://ar.linkedin.com/in/gaspar-suliansky-b1420320a) for the Economics of Financial Markets course (part of the Quantitattive Finace Masters Degree) during our exchange program to the University of Bologna, getting a grade of 30/30 Cum Laude.
We studied daily and monthly returns for different Italian assets, and provided different alocation strategies, such as mean-variance Markowitz optimizatioin and Bayesian methods, including the Black Litterman model and Jorion Shrinkage Estimator. We calculated each of these portfolios and evaluated their performance based on different statistics [using Python](https://github.com/SGreloni/Finance-Portfolio-Optimization-/blob/main/Code-Daily.ipynb)

![Efficient Frontier](Proyectos%20(img)/MVP.png)



## Contacto

E-mail: Santiagogreloni@gmail.com

[Linkedin](https://www.linkedin.com/in/santiago-greloni-4892a9196) 

